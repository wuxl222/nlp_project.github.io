<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 7.0.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-Sharing" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/12/26/Sharing/" class="article-date">
  <time class="dt-published" datetime="2023-12-26T02:08:26.155Z" itemprop="datePublished">2023-12-26</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h3 align='center'> How to apply LLMs in specific domains</h3>

<h5 align='right'>吴晓玲</h5>

<p>Large Language Models (LLMs) have revolutionized the field of Natural  Language Processing (NLP) with their ability to understand and generate  human-like text. They can be fine-tuned and applied in various domains  to perform specific tasks. Here’s a detailed guide on how to apply LLMs  in specific domains, focusing on fine-tuning, training, and inference.</p>
<p><strong>Fine-Tuning LLMs</strong></p>
<ul>
<li>Fine-tuning is a process where a pre-trained model, which has already learned patterns and features on a large dataset, is further trained on a smaller, domain-specific dataset. This method is crucial when you want to adapt a pre-existing model to a specific task or domain. Here’s a step-by-step guide on how to fine-tune LLMs:<ul>
<li><strong>Step 1:</strong> <strong>Define the Task</strong>. Whether it’s sentiment analysis, text summarization, text generation or a  machine translation and so on.</li>
<li><strong>Step 2: Choose a Pre-trained Model.</strong> Select a pre-trained LLM that aligns with your task. Models like GPT-3,  BERT, or RoBERTa are popular choices. Ensure that the chosen model has  the capacity to handle the complexity of your task.</li>
<li><strong>Step3: Dataset Preparation.</strong> Prepare a labeled dataset specific to your task. The dataset should  include examples of inputs and corresponding desired outputs. Ensure  diversity in the data to improve the model’s generalization.</li>
<li><strong>Step4: Data Preprocessing.</strong> Tokenize and preprocess the text data to match the format used during  pre-training. This often involves converting text into numerical tokens  and handling any specific requirements of the chosen model.</li>
<li><strong>Step 5: Model Architecture Adjustment.</strong> Modify the architecture of the pre-trained model to suit your task. For  example, add task-specific layers or adjust the final output layer to  match the number of classes in your classification task.</li>
<li><strong>Step 6: Training Parameters.</strong> Set hyperparameters such as learning rate, batch size, and the number of training epochs. Experiment with different values to find the optimal  configuration for your task.</li>
<li><strong>Step 7: Fine-tune the model.</strong> Train the modified model on your task-specific dataset.</li>
</ul>
</li>
</ul>
<p><strong>Evaluate the fine-tuned model</strong></p>
<ul>
<li>Evaluate the Model: After fine-tuning, evaluate the model’s performance on a separate validation set to ensure it generalizes well to unseen data.</li>
</ul>
<p><strong>Example: Finetune TinyLLaMA-1b with clinical instruction data with QLoRA method, and then evaluate the finetuned LLM on MedMCQA dataset.</strong></p>
<p>original clinical instruction data</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">&quot;input&quot;</span>: <span class="string">&quot;Hello doctor,I had mumps five months ago and after that, I started to have an infection in my left testes. It was swollen and now it has shrunk to almost half the size of the other one. As I am sexually active, I feel a pain in each of the vas deferens after sex. If I do not have sex for days, they become sensitive. I was treated with Ceftum 500 mg, the first time I had an infection. Now my question is, is there any chance that the infection is still in my body? And, do I need to get examined for it? For the time being, please suggest some precautionary antibiotics for my relief.&quot;</span>,</span><br><span class="line"><span class="string">&quot;answer_icliniq&quot;</span>: <span class="string">&quot;Hello, Welcome to Chat Doctor forum. I can understand your concern. You had mumps and this is a viral infection known to cause an inflammation of the testis in some cases. Take care. For more information consult a sexologist online &quot;</span>,</span><br><span class="line"><span class="string">&quot;answer_chatgpt&quot;</span>: <span class="string">&quot;It is possible that you still have an infection in your testes, especially if you have been experiencing pain and sensitivity in your vas deferens. It is recommended that you consult with a doctor or urologist for an examination and further testing to determine if there is an ongoing infection.\n\nAs an AI language model, I cannot prescribe medications or antibiotics. However, taking antibiotics without a doctor&#x27;s prescription is not recommended. It is important to consult with a healthcare professional for proper diagnosis and treatment.&quot;</span>,</span><br><span class="line"><span class="string">&quot;answer_chatdoctor&quot;</span>: <span class="string">&quot;1) There is no need to worry about reinfection as the infection caused by Mumps virus does not persist in the body and is completely cleared up by the immune system.2) The swelling at your scrotum is due to inflammation of epididymis or vas deferens which is a result of mumps infection.3) There is no need to take antibiotics for this problem. You can take anti-inflammatory medicines like Ibuprofen for symptomatic relief.4) Doing cool compresses over the scrotal area will help in reducing the inflammation and pain.&quot;</span>&#125;</span><br></pre></td></tr></table></figure>

<p>processed clinical instruction data</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">&quot;conversation_id&quot;</span>: <span class="number">1</span>,</span><br><span class="line"> <span class="string">&quot;category&quot;</span>: <span class="string">&quot;Brainstorming&quot;</span>,</span><br><span class="line"> <span class="string">&quot;conversation&quot;</span>:</span><br><span class="line"> 	[&#123;<span class="string">&quot;human&quot;</span>: <span class="string">&quot;Hello doctor,I had mumps five months ago and after that, I started to have an infection in my left testes. It was swollen and now it has shrunk to almost half the size of the other one. As I am sexually active, I feel a pain in each of the vas deferens after sex. If I do not have sex for days, they become sensitive. I was treated with Ceftum 500 mg, the first time I had an infection. Now my question is, is there any chance that the infection is still in my body? And, do I need to get examined for it? For the time being, please suggest some precautionary antibiotics for my relief.&quot;</span>, </span><br><span class="line">      <span class="string">&quot;assistant&quot;</span>: <span class="string">&quot;Hello, Welcome to Chat Doctor forum. I can understand your concern. You had mumps and this is a viral infection known to cause an inflammation of the testis in some cases. Take care. For more information consult a sexologist online &quot;</span>&#125;]&#125;</span><br></pre></td></tr></table></figure>

<p>train_args:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="string">&quot;output_dir&quot;</span>: <span class="string">&quot;output/tinyllama-qlora-clinical&quot;</span>,</span><br><span class="line">    <span class="string">&quot;model_name_or_path&quot;</span>: <span class="string">&quot;./tinyllama&quot;</span>,</span><br><span class="line">    <span class="string">&quot;train_file&quot;</span>: <span class="string">&quot;iCliniq_data_new.jsonl&quot;</span>,</span><br><span class="line">    <span class="string">&quot;num_train_epochs&quot;</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="string">&quot;per_device_train_batch_size&quot;</span>: <span class="number">2</span>,</span><br><span class="line">    <span class="string">&quot;gradient_accumulation_steps&quot;</span>: <span class="number">2</span>,</span><br><span class="line">    <span class="string">&quot;learning_rate&quot;</span>: <span class="number">1e-5</span>,</span><br><span class="line">    <span class="string">&quot;max_seq_length&quot;</span>: <span class="number">512</span>,</span><br><span class="line">    <span class="string">&quot;logging_steps&quot;</span>: <span class="number">20</span>,</span><br><span class="line">    <span class="string">&quot;save_steps&quot;</span>: <span class="number">100</span>,</span><br><span class="line">    <span class="string">&quot;save_total_limit&quot;</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="string">&quot;lr_scheduler_type&quot;</span>: <span class="string">&quot;cosine&quot;</span>,</span><br><span class="line">    <span class="string">&quot;warmup_steps&quot;</span>: <span class="number">500</span>,</span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;gradient_checkpointing&quot;</span>: false,</span><br><span class="line">    <span class="string">&quot;disable_tqdm&quot;</span>: false,</span><br><span class="line">    <span class="string">&quot;optim&quot;</span>: <span class="string">&quot;adamw_hf&quot;</span>,</span><br><span class="line">    <span class="string">&quot;seed&quot;</span>: <span class="number">42</span>,</span><br><span class="line">    <span class="string">&quot;fp16&quot;</span>: true,</span><br><span class="line">    <span class="string">&quot;report_to&quot;</span>: <span class="string">&quot;tensorboard&quot;</span>,</span><br><span class="line">    <span class="string">&quot;dataloader_num_workers&quot;</span>: <span class="number">5</span>,</span><br><span class="line">    <span class="string">&quot;save_strategy&quot;</span>: <span class="string">&quot;steps&quot;</span>,</span><br><span class="line">    <span class="string">&quot;weight_decay&quot;</span>: <span class="number">0</span>,</span><br><span class="line">    <span class="string">&quot;max_grad_norm&quot;</span>: <span class="number">1.0</span>,</span><br><span class="line">    <span class="string">&quot;remove_unused_columns&quot;</span>: false</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>train.out:</p>
<p><img src="D:\pro\work\source\image\train.out.png" alt="train.out"></p>
<p>evaluation code:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer, AutoModelForCausalLM</span><br><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">from</span> peft <span class="keyword">import</span> PeftModel</span><br><span class="line"><span class="keyword">import</span> jsonlines</span><br><span class="line">path=<span class="string">&#x27;./tinyllama&#x27;</span></span><br><span class="line">base_model=AutoModelForCausalLM.from_pretrained(path,device_map=<span class="string">&#x27;auto&#x27;</span>)</span><br><span class="line">tokenizer=AutoTokenizer.from_pretrained(<span class="string">&#x27;./tinyllama&#x27;</span>,add_bos_token=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">pretrain_model=PeftModel.from_pretrained(base_model,<span class="string">&#x27;./output/tinyllama-qlora-clinical/final&#x27;</span>)</span><br><span class="line">dataset = load_dataset(<span class="string">&#x27;json&#x27;</span>, data_files=<span class="string">&#x27;dev_data.json&#x27;</span>,split=<span class="string">&#x27;train&#x27;</span>)</span><br><span class="line"></span><br><span class="line">correct_predictions = <span class="number">0</span></span><br><span class="line">total_instances = <span class="built_in">len</span>(dataset)</span><br><span class="line">reply_tinyllamda=&#123;&#125;</span><br><span class="line">output_file_path=<span class="string">&#x27;reply_tinyllama.jsonl&#x27;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> instance <span class="keyword">in</span> tqdm(dataset):</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#将问题和每个选项拼接在一起</span></span><br><span class="line">    question_text = instance[<span class="string">&quot;question&quot;</span>]</span><br><span class="line">    options = [<span class="built_in">str</span>(instance[<span class="string">&quot;opa&quot;</span>]), <span class="built_in">str</span>(instance[<span class="string">&quot;opb&quot;</span>]), <span class="built_in">str</span>(instance[<span class="string">&quot;opc&quot;</span>]), <span class="built_in">str</span>(instance[<span class="string">&quot;opd&quot;</span>])]</span><br><span class="line"></span><br><span class="line">    inputs = [tokenizer.encode(question_text + <span class="string">&quot; &quot;</span> + option, return_tensors=<span class="string">&#x27;pt&#x27;</span>) <span class="keyword">for</span> option <span class="keyword">in</span> options]</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#使用模型进行预测，获取每个预测的最后一个token的logit</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        predictions = [pretrain_model(<span class="built_in">input</span>)[<span class="number">0</span>] <span class="keyword">for</span> <span class="built_in">input</span> <span class="keyword">in</span> inputs]</span><br><span class="line">        logits = [prediction[:, -<span class="number">1</span>, :] <span class="keyword">for</span> prediction <span class="keyword">in</span> predictions]</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#计算每个选项的概率，找到概率最高的选项</span></span><br><span class="line">    probabilities = [torch.nn.functional.softmax(logit, dim=-<span class="number">1</span>) <span class="keyword">for</span> logit <span class="keyword">in</span> logits]</span><br><span class="line">    total_probabilities = [torch.<span class="built_in">sum</span>(probability).item() <span class="keyword">for</span> probability <span class="keyword">in</span> probabilities]</span><br><span class="line">    max_probability_option = options[total_probabilities.index(<span class="built_in">max</span>(total_probabilities))]</span><br><span class="line">    max_index=total_probabilities.index(<span class="built_in">max</span>(total_probabilities))   </span><br><span class="line">    </span><br><span class="line">    reply_tinyllamda[<span class="string">&#x27;question&#x27;</span>]=question_text</span><br><span class="line">    reply_tinyllamda[<span class="string">&#x27;answer_tinyllama&#x27;</span>]=max_probability_option</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#保存结果</span></span><br><span class="line">    <span class="keyword">with</span> jsonlines.<span class="built_in">open</span>(output_file_path, mode=<span class="string">&#x27;a&#x27;</span>) <span class="keyword">as</span> writer:</span><br><span class="line">        writer.write(reply_tinyllamda)</span><br><span class="line">    correct_index = instance[<span class="string">&quot;cop&quot;</span>]-<span class="number">1</span>   </span><br><span class="line">    <span class="keyword">if</span> correct_index == max_index:</span><br><span class="line">        correct_predictions += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">accuracy = correct_predictions / total_instances</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;accuracy: <span class="subst">&#123;accuracy:<span class="number">.2</span>%&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>eval.out:</p>
<p><img src="D:\pro\work\source\image\evaluate.out.png" alt="evaluate.out"></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/12/26/Sharing/" data-id="clqlrpy8e0000awi6b77q8173" data-title="" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  


</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/12/">December 2023</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2023/12/26/Sharing/">(no title)</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2023 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>